\documentclass[letterpaper,12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath,amsthm,amssymb}
\usepackage{fullpage}
\usepackage{graphicx,float,wrapfig,subfig,tabularx,ulem}
\graphicspath{{figure/}}
%\usepackage{csquotes}
\usepackage{color}
\usepackage{natbib}
\usepackage{hyperref}

\newcommand{\ktm}[1]{{\color{red} #1}} %red comments: Karsten
\newcommand{\hh}[1]{{\color{blue} #1}} %blue comments: Heike
\newcommand{\svp}[1]{{\color{yellow}#1}} %yellow: Susan (It's more fun that way - comment roulette)


%opening
\title{Binning Strategies and Related Loss for Large Data}
\author{Karsten Maurer$^1$, Susan VanderPlas$^1$, Heike Hofmann$^{1,2}$\\$^1$Department of Statistics, $^2$Human Computer Interaction\\Iowa State University}

\begin{document}
\maketitle

\begin{abstract}
Dealing with the data deluge of the Big Data Age is both exciting and challenging. The demands of large data require us to re-think strategies of visualizing data. Plots employing binning methods have been suggested in the past as viable alternative to standard plots based on raw data, as the resulting area plots tend to not be affected by increases in data as much. This comes with the price of loss of information inherent to any binning scheme. In this paper we discuss properties of two commonly used binning algorithms. We define loss of information in the specific setting of two dimensional displays, provide readily applicable tools for loss evaluation and discuss the two binning schemes with respect to their loss in the framework of a simulation and two case studies.
\end{abstract}


\section{Introduction}


<<setup,echo=F,include=F,eval=T>>=
library(ggplot2)
library(gridExtra)
library(dbData)
library(reshape2)
#library(multicore)
library(devtools)
library(xtable)
library(RColorBrewer)

connect <- dbConnect(dbDriver("MySQL"), host="mysql2.stat.iastate.edu", 
                     port=3306, user="dbaccess", dbname="baseball")
pitch <- new("dataDB", co=connect, table="Pitching")

d1 <- dbData(pitch, vars=c( "G", "SO")) # Games, Strike Outs

xtd.data <- function(x) {
# replicate each line as often as given by the last entry
  k <- dim(x)[2]

  res <- data.frame(sapply(1:(k-1), function(var) {
			return(rep(x[,var], x[,k]))
		}
	))
	names(res) <- names(x)[1:(k-1)]
	return(res)
}
d1.ext <- xtd.data(d1)
@

<<scatterplots, echo=F,include=F>>=
qplot(data=d1.ext, x=G, y=SO, geom="point", main="Games and Strike Outs in ML Baseball", alpha=I(.05), size=I(3)) + theme_bw() + xlab("Games (Count)")+ ylab("Strike Outs (Count)")
  ggsave("./figure/OverplottingAlpha.pdf", width=5, height=5)
qplot(data=d1, x=G, y=SO, geom="point", main="Games and Strike Outs in ML Baseball", size=I(3)) + theme_bw() + xlab("Games (Count)")+ ylab("Strike Outs (Count)")
  ggsave("./figure/Overplotting.pdf", width=5, height=5)
qplot(data=d1, x=G, y=SO, geom="point", main="Games and Strike Outs in ML Baseball", shape="o", size=I(6)) + scale_shape_identity() + theme_bw() + xlab("Games (Count)")+ ylab("Strike Outs (Count)")
  ggsave("./figure/OverplottingCircles.pdf", width=5, height=5)
qplot(data=d1.ext, x=G, y=SO, geom="hex", fill=..count.., group=1) +  
  scale_fill_gradient("Frequency", low="#56B1F7", high="#132B43", guide="legend", trans="log", breaks=c(1, 10, 100, 1000)) + theme_bw() + xlab("Games (Count)") + ylab("Strike Outs (Count)") + theme(legend.position="bottom")
  ggsave("./figure/HexBinning.pdf", width=5, height=5)
d1.hex.obj <- hexbin(d1.ext$G, d1.ext$SO)
d1.hex <- data.frame(hcell2xy(d1.hex.obj))
names(d1.hex) <- c("G", "SO")
d1.hex$Freq <-  d1.hex.obj@count

qplot(data=d1.hex, x=G, y=SO, geom="point", size=Freq) + scale_size_area("Frequency", trans="log", breaks=c(1, 10, 100, 1000, 10000))+ theme_bw() + xlab("Games (Count)")+ ylab("Strike Outs (Count)") + theme(legend.position="bottom")

qplot(data=d1.hex, x=G, y=SO, geom="point", size=Freq+1) + 
  scale_size_area("Frequency", max_size=4, trans="log", breaks=c(1, 10, 100, 1000, 10000))+ theme_bw() + xlab("Games (Count)")+ ylab("Strike Outs (Count)") + theme(legend.position="bottom")

  ggsave("./figure/BinScatter.pdf", width=5, height=5)

# Sunflower plot
# d1.hex$id <- seq(1:nrow(d1.hex))
# d1.hex2 <-  ddply(d1.hex, .(G, SO, id), function(i) return(data.frame(r = cos((floor(i$Freq/10))*seq(0, 2*pi, .05)), theta=seq(0, 2*pi, 0.05), Freq=i$Freq)))
# ggplot(d1.hex2) + geom_subplot(aes(x=G, y=SO, group=id,  subplot=geom_star(aes(r=r, angle=theta, fill=log(Freq), colour=log(Freq)), r.zero=TRUE)))
@

Technological advances have  facilitated collection and dissemination of large data more and more as records are digitized and our lives are increasingly lived online. According to estimates published by the IDC in 2009 the worldwide digital content  doubles about every 18 months -- in 2011 the digital amount was estimated to be at 1.8 zeta bytes (1 ZB = $2^{70}$ bytes $\approx 10^{21}$ bytes).
This ``Data Deluge" of the Big Data Age (NY Times, Feb 2012) poses exciting challenges to data scientists everywhere: 
\begin{quote}``Itâ€™s a revolution ... The march of quantification, made possible by enormous new sources of data, will sweep through academia, business and government. There is no area that is going to be untouched"\end{quote} (Gary King, Harvard Institute).  

% which challenges?
Data sets with millions of records and thousands of variables are not uncommon. 
Such data sets are often too large to fit into a single computer's working memory, and  single-user machines cannot easily deal with them. We can employ database tools to extract relevant variables, thereby vertically subsetting the data. 
However, with millions of records, it can be very difficult to work with data even at that level. 

\citet{Friedman97} proposed in his paper on data mining and statistics that ``Every time the amount of data increases by a factor of ten, we should totally rethink how we analyze it". The same holds for visualizations.  
With a 100-1000 fold increase in the amount of data, the utility of some of our most used graphical tools, such as scatterplots, deteriorates quickly \citep{gold}. 

% binning as a way out
Area plots, such as histograms, do not tend to be as affected by increases in the amount of data as plots that utilize raw data. 
By using binning strategies and the principles for displaying information in area plots, scatterplots can again become useful instruments for large data settings \citep{gold}.

In this paper we describe first the problem scatterplots are exposed to in large-data situations. We discuss two different binning methods and introduce the  {\it loss of information} inherent to binning. We conclude with a detailed example. 

% It is important to be able to describe the contents of the database in a quick and easily comprehensible manner, similar to the use of the mean and standard deviation to describe univariate data, or the mean, and variance matrix to describe multivariate data. Graphical summaries of the data contained within the database are likely to more fully describe large, complex data sets which may not originate from a single multivariate distribution than a set of numerical summaries (i.e. the mean, median, mode, and range used in introductory statistics classes).


\section{Scatterplots for Large Data Sets}
In the case of medium sized data, scatterplots are great tools for showing relationships in two dimensions. For large data, scatterplots suffer from over-plotting -- i.e. more and more points are drawn in close-by places, thereby masking relevant structure.
Figure~\ref{fig:scatter-alpha} shows an example taken from baseball statistics. The scatterplot shows 139 seasons (from the years of 1871 -- 2009) of  pitching statistics for every baseball pitcher as published in Sean Lahman's Baseball database (\url{http://www.seanlahman.com/baseball-archive/}).  The number of games played in a season is plotted versus number of strikeouts a pitcher threw over the course of a season. While the data set is only medium sized with \Sexpr{dim(d1.ext)[1]} observations, it already shows some of the break-down patterns scatterplots experience with large data.
Figure~\ref{fig:scatter-alpha} shows a traditional scatterplot on the left. Each observation is drawn with a filled circle. A triangular structure is apparent with some outliers at a medium number of games and high number of strikeouts. Tukey \citep{tukey} suggested the use of open circles (see Figure~\ref{fig:scatter-alpha}b) to mitigate the problem of over-plotting. Open circles make points visible that are close together. This technique is  not suitable to determine the magnitude, but gives more information than is available with filled points. 
%\hh{We need to decide on one way for referring to $\alpha$-blending in one way - we've got about four different ways of writing it right now. I'm ok with any of them, but we need to stick to it.}
A modern alternative to open circles is alpha blending (see Figure~\ref{fig:scatter-alpha}c). Alpha blending provides  more frequency information, allowing a distinction of higher-resolution density.

\begin{figure}[hbtp]
  \subfloat[Overplotted data ]{\includegraphics[keepaspectratio=TRUE,width=.31\textwidth]{Overplotting.pdf}}
  \subfloat[Tukey-style open circles]{\includegraphics[keepaspectratio=TRUE,width=.31\textwidth]{OverplottingCircles.pdf}} 
	\subfloat[Alpha blending]{\includegraphics[keepaspectratio=TRUE,width=.31\textwidth]{OverplottingAlpha.pdf}}
  
  \subfloat[Hexagonal binned scatterplot]{\includegraphics[keepaspectratio=TRUE,width=.48\textwidth]{HexBinning.pdf}}
  \subfloat[Hexagonal binned bubble plot]{\includegraphics[keepaspectratio=TRUE,width=.48\textwidth]{BinScatter.pdf}} 

	\caption{\label{fig:scatter-alpha} Scatterplots of Games versus Strikeouts in Major League Baseball, using different strategies of dealing with the issue of over-plotting:  (a) uses standard, opaque, filled circles, (b)  uses Tukey's recommended open circles, and (c) uses filled circles with alpha blending ($\alpha$=0.05). Plots (d) and (e) show hexagonal binning strategies with frequency mapped to color and area respectively}
\end{figure}

All of these methods fall short in the example.
As can be seen in Figure~\ref{fig:scatter-alpha}, strategy (a) is the least effective, as it provides information about the outliers and range of the data but cannot provide any point density information. Tukey's open circles (b) help some, but are as prone to over-plotting as solid circles when the data set is large. Alpha blending (c) highlights the structure, but minimizes the visual impact of outliers. The data set is large enough that neither alpha blending nor open circles are completely effective, and so we must pursue a different strategy which can provide better information about the relative density of points at a given location.

%\hh{discussion of sufficient statistics for graphics could go here. Idea: the problems with large data in scatterplots all arise from overplotting, which is a form of implicit data aggregation. In order to keep track of overplotting, we switch to a weighted form of displays and explicitly keep track of overplotting. The aggregated data forms the sufficient statistics for a scatterplot, i.e. provide all of the information necessary to render it. The scatterplot therefore does not display more than that information. Making the data aggregation explicit allows us to calculate the loss we experience.}

One approach to reduce the graphical complexity is to bin the displayed data. This has the additional advantage of reducing the size of the stored data, as only the bin centers and the frequency of points which correspond to that bin must be stored.

Histograms are an example of plots which use binned variables. To extend histograms to a graph which can represent the joint distribution between two variables, it is a natural step to form a tesselated grid on a two dimensional Cartesian plane and use some other attribute (color, 3D rendering) to provide joint density information within each grid cell, known as a tile. A {\it binned scatterplot} uses shading to provide frequency information, with tiles (rather than bars in a histogram) centered at the bin center, rather like a two-dimensional histogram viewed from above. 

Methods commonly used to display binned variables include sunflower plots \citep{sunflowerplots}, modified scatterplots, and kernel density smoothing of tonal variation \citep{martin-gold}. Sunflower plots are scatterplots of binned data, where the symbol used for the bin increases in complexity in proportion to the number of points in that bin. Sunflower plots are particularly useful when the number of points in each bin remains reasonably small.  

When the point area is scaled in proportion to frequency, scatterplots can be used to display the binned data as well.  When points are filled circles, these plots are also known as ``bubble plots", which were first used by William Playfair \citep{playfair, playfair2}. Kernel density smoothing can be used to vary $\alpha$ or color according to a smoothed density, providing features similar to binned scatterplots or alpha blended scatterplots in a more smooth, continuous fashion. These estimates require parameter tuning and may hide gaps in the data by over-smoothing while simultaneously de-emphasizing outlying points.

As alternatives, Figure~\ref{fig:scatter-alpha} (d-e) contains examples of a hexagonally binned scatterplot with frequency encoded as color (d) and a ``bubble plot" (referred to as a modified scatterplot) with frequency encoded as point size (e). The hexagonal tiles and the modified scatterplot are more effective at displaying the shape of the joint density and preserving outliers than any of the scatterplots shown in Figure~\ref{fig:scatter-alpha} (a-c). The binned scatterplot is less prone to the Hermann-grid illusion %\hh{need a reference} 
than the bubble plot, particularly for binned data, where bin centers usually fall on a grid.

% \begin{figure}[hbtp]
% <<conclusions, out.width='\\textwidth', fig.width=5, fig.height=3>>=
% library(dbData)
% connect <- dbConnect(dbDriver("MySQL"), user="2009Expo",
% password="R R0cks", port=3306, dbname="baseball",
% host="headnode.stat.iastate.edu")
% pitch <- new("dataDB", co=connect, table="Pitching")
% pitch.stats <- dbData(vars=list("G", "SO", "yearID"), pitch)
% qplot(yearID, SO/G, alpha=I(0.2), weight=Freq, data=pitch.stats, colour=G) + xlab("season")
% @
% \caption{\caption{strikeout-rate}Scatterplot of rate of strikeouts per game over time. Each dot corresponds to a pitcher's seasonal average. Coloring indicates number of games played, alpha-blending is used additionally to emphasize high-density regions. There's clear break points in the strikeout rates over time. In 1902 XXXX}
% \end{figure}
Only in the binned scatterplot and the bubble plot the inner structure of the data becomes apparent: 
the joint density consists two distinct ridges following two lines with very different slopes. The lower slope corresponds to the modern average strike out rate of pitchers of just under one strike-out per game. The other line has a slope of about four times that rate. This high rate is also associated with fewer games played. Closer investigation of other, related variables reveals that this high strike-out rate corresponds mainly to historic pitchers with much shorter seasons (in 1876 only 70 games were played in a season, as opposed to 162 in 2009), and qualitatively different balls and  bats.

% two centers - almost figure~\ref{strikeout-rate} is numerically loss-free (binning is to integers, reduces the data by 16\%)

%\hh{... This is the place to put some background literature: include the discussion of use of binning in the literature: Martin Theus's chapter in the GOLD book, Dan Carr's hexagonal binning, sunflower plots, ... }  

For extremely large data sets, binned scatterplots are a more useful visualization of two-dimensional density information than the scatterplot, and are less computationally demanding, as not every single point in the data set has to be  rendered separately. 

As with histograms, the width of bins (or the number of bins) is an important factor in the detail of the binned data and the resulting plot: if the bin width is too small in comparison to the amount of data available, there is little advantage to binning, but if the bin width is too large, interesting features of the joint distribution may be obscured by over-smoothing. 




%\hh{Main motivation: we want to investigate how much information we lose when we bin.}

%\hh{I also would like to pick up the idea of sufficient statistics for scatterplots again - as a mathematical basis for measuring the amount of information that goes into a plot (since we can't expect to ever get more out of a plot). .... I'm not sure, yet, where in the paper we should place this discussion.}


\section{Binning Data}
We will only consider binning in up to two dimensions, $X$ and $Y$. The algorithms we discuss are immediately applicable to higher dimension, but we do not feel that the paper would benefit from a more general discussion.
Binning in dimensions $X$ and $Y$ provides us with a more condensed form of the data that ideally preserves both the joint distribution as well as the margins, while reducing the amount of information to a fraction of the original. 

Binning is a two-step procedure: we first assign each data tuple $(x, y)$ to a bin center $(x^\ast,y^\ast)$, and in a second step we find the frequency of each bin center, resulting in  triples  of the form $(x^\ast, y^\ast, c)$, where $c$ is the number of all observations assigned to bin center $(x^\ast,y^\ast)$.

 We will proceed with rectangular bins for simplicity, but other binning schemes, such as those which utilize hexagonal bins are also common \citep{scatterplots}. Rectangular bins are advantageous because bins in $x$ and $y$ are orthogonal to each other, thus, we can present the one-dimensional case which will easily generalize to two dimensions.


\subsection{Standard Rectangular Binning Algorithm}
We will first define a binning algorithm that results in observed values being allocated to equal sized bins in a deterministic manner. Let $x_{(i)}$ correspond to the ordered values of $X$, where $i=1, ..., n$ and $n$ is the number of total observations. Then $x_{(1)}$ and $x_{(n)}$ then denote the observed minimum and the maximum of $X$, respectively. 

We then define a set of bin centers $\{ x_j^\ast \}$, with $1 \le j \le N_X$, where $N_X$ is the overall number of bins in dimension $X$. Binning can be determined by the number of bins, $N_X$, which then dictates the bin width, $\omega_X$. For standard binning the relationship between these two parameters is  given as 
\[
\omega_X = (x_{(n)} - x_{(1)})/N_X
\]

Mathematically, binning corresponds to a function $b_X(.)$ that assigns to a value $x_i$ the closest bin center $x^\ast_i \in \{ x_j^\ast \}, j \in \{1, ..., N_X\}$, i.e. the binning function   $b_X(.) : x_i \rightarrow x^\ast_i$ is defined as 
% \begin{center}
% $x^\ast_i = b_X(x_i) = x_{(1)}^\ast + \omega_X \cdot \left\lfloor (x_i-x_{(1)})/\omega_x \right\rfloor$ \hspace{.5cm} $\forall x_i < x_{(n)}$, and \\ 
% \vspace{.03}
% $x^\ast_i = b_X(x_i) = x_{(n)} - \omega_x/2$ \hspace{.5cm} $\forall x_i = x_{(n)}$
% \end{center}
\[ 
 x^\ast_i = b_X(x_i) =
  \begin{cases}
    x_{(1)}^\ast + \omega_X \cdot \left\lfloor (x_i-x_{(1)})/\omega_x \right\rfloor & \quad \forall x_i < x_{(n)} \\
    x_{(n)} - \omega_x/2 & \quad \forall x_i = x_{(n)}
  \end{cases}
\]

where $x_{(1)}^\ast = x_{(1)} + \omega_x/2$ is the center of the first bin and $\lfloor . \rfloor$ denotes the floor function, $\lfloor x \rfloor$ is the largest integer value smaller than $x$. An intuitive understanding of the formula is developed by noting $\left\lfloor (x_i-x_{(1)})/\omega_x \right\rfloor$ represents the number of bins centers that lie between $x_{(1)}$ and $x_i$.  Thus, the binning function is finding the location of the bin center for $x_i$ by adding the distance between the bin centers for $x_{(1)}$ and $x_i$ to the known bin center for $x_{(1)}$.\\

Note that the binning function above amounts to dividing up the range of data by a specified number of bins of equal width. A more flexible binning algorithm may be achieved if we allow the specification of the first bin center and bin width instead of the number of bins. This strategy would still fit into the framework of assigning values to the closest bin center. 


Figure~\ref{fig:BinningTutorial} provides an illustration of the binning process extended to a two dimensional situation.
<<binningDemo,echo=F,include=F,cache=T>>=
      library(mvtnorm)
      set.seed(33)
      d <- rmvnorm(200, c(5, 25), matrix(c(100, 50, 50, 100), nrow=2))
      d <- data.frame(d)
      names(d) <- c("x", "y")
      d <- subset(d, (x>-20 & x<40 & y>0 & y<50),drop=TRUE)
      d.rand <- d
      dnew <- binStd(cbind(d-5, Freq=1), c(10, 10))
      dnew$x <- dnew$x+5
      dnew$y <- dnew$y+5
      d$bin <- dnew$x%in%c(5,15) & dnew$y==25
      d$a <- .33 + .66*d$bin
      d.orig <- d

      d$xbin <- NA
      d$ybin <- NA
      idx <- which(dnew$x%in%c(5, 15) & dnew$y==25)
      d$xbin[idx] <- dnew$x[idx]
      d$ybin[idx] <- 25

      temp <- data.frame(   
                x=c(-20, -10,   0,  10,  20,  30,  40, -20, -20, -20, -20, -20, -20), 
             xend=c(-20, -10,   0,  10,  20,  30,  40,  40,  40,  40,  40,  40,  40), 
                y=c(  0,   0,   0,   0,   0,   0,   0,   0,  10,  20,  30,  40,  50), 
             yend=c( 50,  50,  50,  50,  50,  50,  50,   0,  10,  20,  30,  40,  50), 
            group=c(  1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13))
      qplot(data=d[,], x=x, y=y, alpha=a,  geom="point", 
            xlim=c(-20, 40), ylim=c(0, 50)) +
        annotate("polygon", x=c(10, 10, 20, 20), y=c(20, 30, 30, 20), 
                 fill="blue", alpha=.25) +
        annotate("polygon", x=c(0, 0, 10, 10), y=c(20, 30, 30, 20), 
                 fill="green", alpha=.25) +
        scale_alpha(range=c(.5, 1), guide=FALSE) +
        scale_colour_identity(guide=FALSE) + 
        scale_shape(solid=T,guide=FALSE) +
        theme_bw() +
        annotate("point", x=rep(seq(-15, 35, 10),each=7), 
               y=rep(seq(-5, 55, 10), times=6), 
               shape='+', size=8, colour="gray90") + 
        geom_point(data=d[which(!is.na(d$xbin)),], 
                   aes(x=x, y=y, shape=factor(xbin), colour=factor(xbin)), 
                   size=I(3), aes.inherit=FALSE) + 
        scale_shape_manual(name="Bin", values=c("5"=16, "15"=15), 
                           labels=c("(5,25)", "(15, 25)"),guide=FALSE) +
        scale_colour_manual(values=c("5"="green", "15"="blue"), name="Bin", 
                            labels=c("(5,25)", "(15, 25)"),guide=FALSE)+
        annotate("segment", x=temp$x, y=temp$y, 
                 xend=temp$xend, yend=temp$yend, group=temp$group)  + ylim(c(0, 50))
      ggsave("./figure/Binning-DataAndBin.pdf", width=5, height=5, units="in")
      
      d.orig$bin <- dnew$x==15 & dnew$y==25
      d.orig$a <- .33+.66*d.orig$bin
      d.orig$l <- "Data outside bin"
      d.orig$l[d.orig$bin] <- "Binned Data"
      d.orig$size <- 2
      d.orig$size[d.orig$bin] <- 3
      d.orig <- rbind(d.orig, data.frame(x=15, y=25, a=1, l="Visual Center", size=2,bin=TRUE),
        data.frame(x=mean(d.orig$x[d.orig$bin]), y=mean(d.orig$y[d.orig$bin]),                                                                                    
        l="Numerical Center", a=1, size=2, bin=TRUE))
      ggplot(data=d.orig, aes(x=x, y=y, shape=l, size=l, colour=l)) + 
        annotate("polygon", x=c(10, 10, 20, 20), y=c(20, 30, 30, 20), 
                 fill="blue", alpha=.10) + 
        geom_point() +
        scale_shape_manual(name="", values=c("Data outside bin" = 16, 
                                             "Binned Data" = 15, 
                                             "Numerical Center" = 17, 
                                             "Visual Center" = 18)) +
        scale_colour_manual(name="", values=c("Data outside bin" = "grey60", 
                                              "Binned Data" = "blue",
                                              "Numerical Center" = "blue4", 
                                              "Visual Center" = "blue4"))+
        scale_size_manual(name="", values=c("Data outside bin" = 3, "Binned Data"=3,
                                            "Numerical Center" = 5, 
                                            "Visual Center" = 5))+
        theme_bw() +
        annotate("point", x=15, y=25, shape=18, size=5, colour="blue4") + 
        annotate("point", x=d.orig$x[which(d.orig$l=="Numerical Center")], 
                 y=d.orig$y[which(d.orig$l=="Numerical Center")], shape=17, size=5, 
                 colour="blue4") +
#        annotate("segment", x=temp$x, y=temp$y, xend=temp$xend, yend=temp$yend, group=temp$group) + 
#         annotate("text", x=d$x[which(d$l=="Numerical Center")], 
#                  y=d$y[which(d$l=="Numerical Center")], 
#                  label=expression(paste('(', over(phantom(0),x)[i], ',', over(phantom(0),y)[i],')'))) +
        scale_x_continuous(breaks=c(10, 15, 20), limits=c(8, 22), labels=c(expression(x^{"*"}-b[x]/2), expression(x^{"*"}), expression(x^{"*"}+b[x]/2)), name='') + 
        scale_y_continuous(breaks=c(20, 25, 30), limits=c(18, 32), labels=c(expression(y^{"*"}-b[y]/2), expression(y^{"*"}), expression(y^{"*"}+b[y]/2)), name='')
      ggsave("./figure/Binning-DataVisualNumericalCenter.pdf", width=5, height=4, units="in")
@

\begin{figure}[hbtp]
  \centering
  \subfloat[Standard Algorithm]{\includegraphics[keepaspectratio=true, width=3in]{./figure/Binning-DataAndBin.pdf}}
%   \subfloat{\includegraphics[keepaspectratio=TRUE, width=2in]
%   {./figure/RandomBinningDataPoints1}}
  \subfloat[Random Algorithm]{\includegraphics[keepaspectratio=TRUE, width=3in]
  {./figure/RandomBinningDataPoints1.pdf}}
  \caption{Points are assigned to bins centered at (5, 25) an (15,25). Points in the standard algorithm are contained within the defined bin region, where points from the random algorith may be assigned to bins from outside the defined bin regions.}
  \label{fig:BinningTutorial}
\end{figure}

\subsection{Random Binning Algorithm}
<<RandomBinDemo,echo=F,include=FALSE>>=
nl <- ncol(d.rand)
dnew <- d.rand
binning <- c(10, 10)
for(x in 1:(nl)){
  y <- d.rand[,x] - binning[x]/2
dnew[,x] <- binning[x]*(floor(y/binning[x]) + 
  sapply((y%%binning[x])/binning[x], function(p) rbinom(1, 1, p))) + binning[x]/2
}
temp <- data.frame(   
          x=c(-20, -10,   0,  10,  20,  30,  40, -20, -20, -20, -20, -20, -20), 
       xend=c(-20, -10,   0,  10,  20,  30,  40,  40,  40,  40,  40,  40,  40), 
          y=c(  0,   0,   0,   0,   0,   0,   0,   0,  10,  20,  30,  40,  50), 
       yend=c( 50,  50,  50,  50,  50,  50,  50,   0,  10,  20,  30,  40,  50), 
      group=c(  1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13))

dnew$id <- interaction(dnew[,1:(nl)])
idx <- which(dnew$x%in%c(5, 15) & dnew$y==25)
d.rand$bin <- dnew$x%in%c(5, 15) & dnew$y==25
d.rand$a <- .33 + .66*d.rand$bin
d.rand$xbin <- NA
d.rand$ybin <- NA
d.rand$xbin[idx] <- dnew$x[idx]
d.rand$ybin[idx] <- 25
# res <- ddply(dnew, .(id), summarize, n=length(id), fsum=sum(Freq))
qplot(data=d.rand[,], x=x, y=y, alpha=a,  geom="point", xlim=c(-20, 40), ylim=c(0, 50)) +
  annotate("polygon", x=c(10, 10, 20, 20), y=c(20, 30, 30, 20), fill="blue", alpha=.25) +
  annotate("polygon", x=c(0, 0, 10, 10), y=c(20, 30, 30, 20), fill="green", alpha=.25) +
  scale_alpha(range=c(.5, 1), guide=FALSE) +
  scale_colour_identity(guide=FALSE) + 
  scale_shape(solid=T,guide=FALSE) +
  theme_bw() +
  annotate("point", x=rep(seq(-15, 35, 10),each=7), 
         y=rep(seq(-5, 55, 10), times=6), 
         shape='+', size=8, colour="gray90") + 
  geom_point(data=d.rand[which(!is.na(d.rand$xbin)),], 
             aes(x=x, y=y, shape=factor(xbin), colour=factor(xbin)), size=I(3), aes.inherit=FALSE) + 
  scale_shape_manual(name="Bin", values=c("5"=16, "15"=15), labels=c("(5,25)", "(15, 25)"),guide=FALSE) +
  scale_colour_manual(values=c("5"="green", "15"="blue"), name="Bin", labels=c("(5,25)", "(15, 25)"),guide=FALSE)+
  annotate("segment", x=temp$x, y=temp$y, xend=temp$xend, yend=temp$yend, group=temp$group) + ylim(c(0, 50))
ggsave("./figure/RandomBinningDataPoints2.pdf",width=5, height=5)
@
An alternative to the standard binning processes discussed above, is the random binning algorithm which results in statistically nice properties. A random binning algorithm utilizes a non-deterministic bin function $b^r(\cdot)$ which maps value $x$ to one of several bin centers $x^\ast$ with probability $p$. In this paper, we will consider the simplest case of just two bins, so that without loss of generality we can assume that $x$ lies between bin centers $x^\ast_j$ and $x^\ast_{j+1}$. The bin function assigns $x$ to bin center $x^\ast$ with a probability depending on the distance to that bin center;  the closer a value is to a bin center, the higher the probability that the value is assigned to this bin center. 
More formally,
\begin{eqnarray}\label{randbin}
b^r(x) = \left\{\begin{array}{ll} 
  x^\ast_j & \text{ with probability } p = {(x^\ast_{j+1} - x)}/b_X \\
  x^\ast_{j+1} &\text{ with probability } 1-p = {(x - x^\ast_{j})}/b_X
  \end{array}\right.
\end{eqnarray}

% In this manner, the probability of assigning point $x$ to bin $x^\ast_j$ is inversely proportional to the distance between the bin center of bin $x^\ast_j$ and $x$, that is, the closer $x$ is to a bin center, the higher the probability that $x$ will be assigned to that bin. 
This method is easily extensible to also map $x$ into one of more than two bins and can accommodate non-uniform bin sizes, unlike the standard algorithm. 

%To take up the previous number example again, under random binning the values 1.35, 1.55, and 2.35 are assigned to bin centers $\{1, 2, 3\}$ non-deterministic. 
%Value 1.35 has a 35\% probability to be assigned to bin center 2, and accordingly is assigned to bin center 1 with probability 65\%. 
%1.55 is closer to the boundary between bin centers and has therefore a 45\% or 55\% chance to be assigned to bin centers 1 or 2, respectively.

%Figure~\ref{fig:BinningTutorial} illustrates one possible outcome of the random binning process.

The deterministic standard binning algorithm is an example of a ``direct" binning algorithm, in which all points are assigned with weight one to the nearest bin. ``Linear" binning  \citep{martin-gold} is a \textit{computationally intensive} alternative to direct binning  in which adjacent bins are assigned a weight  depending on the distance from the point to that bin, where all weights sum to one.  With large data sets, the calculations required for linear binning become unwieldy, but the random binning algorithm can be considered an approximation to linear binning. Specifically, the expectation of the random binning algorithm is the same as for linear binning.
%However, linear binning requires evaluation of this expectation, whereas the random binning algorithm presented here uses a random sample from the underlying distribution, which is much more computationally tractable.
In the next section, we examine the loss of visual and numerical information due to binning.
% linear binning in Gold book - random algorithm as a less computational method of linear binning


\section{Loss due to Binning}
While binning data has some notable advantages, it does result in loss of some information. Using a small number of bins removes much of the relevant information in the data, while an extremely large number of bins may not reduce the data size or complexity to a sufficient degree. 
Figure~\ref{binning} gives an overview of a data set and binned representations using different numbers of bins, demonstrating the loss of information with increasing bin size.


\begin{figure}[hbtp]
<<binning_setup, echo=FALSE,fig.width=2, fig.height=2, out.width='\\linewidth'>>=
set.seed(46)
x <- round(rnorm(200), digits=2)
y <- round(rnorm(200), digits=2)
library(ggplot2)
X <- data.frame(x,y)
library(plyr)
@
\begin{minipage}{.35\linewidth}
<<first, echo=FALSE, out.width='\\linewidth',fig.width=4, fig.height=4>>=
qplot(x,y, asp=1, main='Unbinned Data')+theme_bw()
@
\end{minipage}
\hfil\begin{minipage}{.645\linewidth}
\begin{minipage}{.495\linewidth}
<<bin001, echo=FALSE, out.width='\\linewidth',fig.width=4, fig.height=5>>=
binwidth=0.1
X$xbin <- round(X$x/binwidth,0)*binwidth
X$ybin <- round(X$y/binwidth,0)*binwidth
Xsmall <- ddply(X, .(xbin, ybin), summarize, count=length(xbin))
qplot(x,y, asp=1, geom="blank") + geom_tile(binwidth=binwidth, aes(x=xbin, y=ybin, fill=count), data=Xsmall) + ggtitle(sprintf("binwidth = %.2f", binwidth)) + geom_vline(aes(xintercept=seq(min(X$xbin)-binwidth, max(X$xbin), by=binwidth)+binwidth/2), colour="grey90") + geom_hline(aes(yintercept=seq(min(X$ybin)-binwidth, max(X$ybin), by=binwidth)+binwidth/2), colour="grey90") + theme_bw()+ theme(legend.position="bottom") +scale_fill_gradient(low="#56B1F7", high="#132B43", guide="legend", trans="log", breaks=c(1, 2, 5, 10, 20))
@
\end{minipage}\hfil
\begin{minipage}{.495\linewidth}
<<bin025, echo=FALSE, out.width='\\linewidth',fig.width=4, fig.height=5>>=
binwidth=0.25
X$xbin <- round(X$x/binwidth,0)*binwidth
X$ybin <- round(X$y/binwidth,0)*binwidth
Xsmall <- ddply(X, .(xbin, ybin), summarize, count=length(xbin))
qplot(x,y, asp=1, geom="blank") + geom_tile(binwidth=binwidth, aes(x=xbin, y=ybin, fill=count), data=Xsmall) + ggtitle(sprintf("binwidth = %.2f", binwidth)) + geom_vline(aes(xintercept=seq(min(X$xbin)-binwidth, max(X$xbin), by=binwidth)+binwidth/2), colour="grey80") + geom_hline(aes(yintercept=seq(min(X$ybin)-binwidth, max(X$ybin), by=binwidth)+binwidth/2), colour="grey80")+ theme_bw()+ theme(legend.position="bottom") +scale_fill_gradient(low="#56B1F7", high="#132B43", guide="legend", trans="log", breaks=c(1, 2, 5, 10, 20))
@
\end{minipage}\\
\hfil\begin{minipage}{.495\linewidth}
<<bin05, echo=FALSE, out.width='\\linewidth',fig.width=4, fig.height=5>>=
binwidth=0.5
X$xbin <- round(X$x/binwidth,0)*binwidth
X$ybin <- round(X$y/binwidth,0)*binwidth
Xsmall <- ddply(X, .(xbin, ybin), summarize, count=length(xbin))
qplot(x,y, asp=1, geom="blank") + geom_tile(binwidth=binwidth, aes(x=xbin, y=ybin, fill=count), data=Xsmall) + ggtitle(sprintf("binwidth = %.2f", binwidth)) + geom_vline(aes(xintercept=seq(min(X$xbin)-binwidth, max(X$xbin), by=binwidth)+binwidth/2), colour="grey70") + geom_hline(aes(yintercept=seq(min(X$ybin)-binwidth, max(X$ybin), by=binwidth)+binwidth/2), colour="grey70")+ theme_bw()+ theme(legend.position="bottom") +scale_fill_gradient(low="#56B1F7", high="#132B43", guide="legend", trans="log", breaks=c(1, 2, 5, 10, 20))
@
\end{minipage}\hfil
\begin{minipage}{.495\linewidth}
<<bin1, echo=FALSE, out.width='\\linewidth',fig.width=4, fig.height=5>>=
binwidth=1
X$xbin <- round(X$x/binwidth,0)*binwidth
X$ybin <- round(X$y/binwidth,0)*binwidth
Xsmall <- ddply(X, .(xbin, ybin), summarize, count=length(xbin))
qplot(x,y, asp=1, geom="blank") + geom_tile(binwidth=binwidth, aes(x=xbin, y=ybin, fill=count), data=Xsmall) + ggtitle(sprintf("binwidth = %.2f", binwidth)) + geom_vline(aes(xintercept=seq(min(X$xbin)-binwidth, max(X$xbin), by=binwidth)+binwidth/2), colour="grey60") + geom_hline(aes(yintercept=seq(min(X$ybin)-binwidth, max(X$ybin), by=binwidth)+binwidth/2), colour="grey60")+ theme_bw()+ theme(legend.position="bottom") +scale_fill_gradient(low="#56B1F7", high="#132B43", guide="legend", trans="log", breaks=c(1, 2, 5, 10, 20))
@
\end{minipage}
\end{minipage}
\caption{\label{binning}Series of scatterplots showing the original data (scatterplot, left), and versions of the binned data for different bin widths. The visual loss from binning at 0.1 is minimal, while a bin width of 1 gives a rough approximation.}
\end{figure}
In the first set of binned data, the bins and the points on the scatterplot are nearly identical, but the scatterplot contains information about overlapping points. The second and third sets of binned data, with bin width=0.25 and 0.50 respectively, show higher-level summaries of the data that contain some numerical and visual loss but which may also provide more visually accessible information about the shape of the two-dimensional density between $x$ and $y$. The fourth set of binned data with bin width = 1.0 is nearly unrecognizable, because the bins are large enough that they provide very little additional information. 


Loss of information occurs on multiple levels during the binning and rendering process. We distinguish three sources:
\begin{itemize}

\item {\it Numerical Loss}, $L^\text{N}$, occurs when points in the data set are reduced to a single points at the bin means.  Allocating points to the numerical center, or the mean of the data corresponding to that bin, causes a loss of information about the exact location of the points.

\item {\it Visual Loss}, $L^\text{V}$, is an implicit loss due to the rendering;  visually we perceive the center of a bin to be the center of the tile. This visual center will generally different from the numerical mean, causing some bias in the perception. Thus visual loss for a bin, $L_i^\text{N}$, is the euclidean distance between the visual center for the $j^{th}$ bin $(x_j^\ast, y_j^\ast)$ and the numerical center $(\bar{x}_j, \bar{y}_j)$. The visual loss is the sum of these bin distances.

\item {\it Frequency Loss} results from our inability to render the frequency information accurately. Different rendering methods for the frequency information will lead to different losses. For the remainder of the paper we will assume that we are using colors in binned scatterplots to represent this information. The number of colors that can be differentiated and mapped back to frequency information accurately is fairly limited. Note that even though this loss will turn out to be substantial, it is, in fact, a huge gain with respect to the original scatterplot, where frequency information is masked in large data situations due to over-plotting of points.

% 
% , so we must balance perceptual limits of frequency information with the limits of overplotting single points. 


% which may not be the center of the data within the bin, causing bias and numerical loss.
% 
% Visual loss occurs in the graphical display of the data, as individual points are represented as tiles, and tiles are perceived according to their visual center, rather than according to the center of the data.
% 
%  
\end{itemize}

We explore the different types of loss in the remainder of this section in more detail. For clarity we will again be specifying loss using notation for a one dimensional binning process. This notation can then be extended to higher dimensions in applications.

\subsection{Numerical and Visual Loss}
Reducing data points $\mathbf{x}$ to the visual bin centers $\mathbf{x}^\ast$ results in a loss of information, as there is a reduction in the variability of the data when data points are transformed from individual values to bin centers on a grid.
If the visual bin centers are the same as the numerical centers for each bin, then any loss could be strictly attributed to difference between points and the bin centers. 
The problem is that the visual center and the numerical center for bins will rarely match, which introduces  visual bias  to the graphical representation of the data, as we perceive the bin center to be the physical middle of the symbol representing the data.

Therefore we distinguish two contributions to the total loss. 
Numerical loss results from the binning process as points are reduced to the numerical center point. The loss for an individual point, $L_i^\text{N}$, is the euclidean distance between the $i^{th}$ data point $(x_i,y_i)$ and the corresponding bin numerical center $(\bar{x}, \bar{y})$. The overall numerical loss is thus the sum of numerical losses for individual points ($L^\text{N} = \sum_{i=1}^n L_i^\text{N}$).
Visual loss is due to the distance between the numerical centers and the visual centers. The visual loss for each bin, $L_j^\text{V}$, is the euclidean distance between the visual center for the $j^{th}$ bin $(x_j^\ast, y_j^\ast)$ and the numerical center $(\bar{x}_j, \bar{y}_j)$. The overall visual loss is the sum of these bin distances ($L^\text{V} = \sum_{j=1}^{N_x} L_j^\text{V}$). 
This discrepancy is particularly clear as demonstrated in Figure~\ref{fig:stdBinTutorial2}, in which the visual center of the bin varies significantly from the numerical center of the data contained within the bin under the standard algorithm.  
This discrepancy between the visual center of the bin and the numerical center of the data within the bin occurs in both binning algorithms, and we can note that the loss functions napply to either approach to binning.

The magnitude of the numerical and visual losses are clearly dictated by the scale on which the data was recorded, thus and the loss values have no direct interpretation. However, we can examine both the numerical and the visual loss from the perspective of proportion of the greatest possible loss.  
To do this for numerical loss we consider the binning scenario that would always lead to greatest possible loss; allocating all data points into a single bin.  Thus we can define the greatest possible numerical loss as

\begin{center}
$ L_0^\text{N} = \sum_{i=1}^{n} \sqrt{(x_i-\bar{x})^2 + (y_i-\bar{y})^2}  $, 
\end{center}

where $\bar{x}$ and $\bar{y}$ are the sample averages for x and y respectively.  After we select a binning strategy and binwidth we can then assess the proportion of possible numerical loss incurred due to binning by computing $L^\text{N} / L_0^\text{N}$.

%-------
% Cut "total loss" stuff, because this was all a relic of "spatial loss" part of paper
%Let us denote the difference between point $x_i$ from its bin center as $S_i = \left|x_i-b(x_i)\right|$ for each point in the data set $i=1, ..., n$.
%We may then combine these losses from the entire data set to be defined as 
%\[L_s(\mathbf{x}) = 1/S^2_{\emptyset} \cdot \sum_{i=1}^{n} S^2_i\] 
%where $S_\emptyset^2$ is the loss which results when the entire data set is reduced to a single bin,
%$S^2_{\emptyset} = \sum_i (x_i-\tilde{x})^2$ for the single bin center $\tilde{x}$.
%The loss depends on the choice of bin center: candidates for bin centers are the mean of the points within the bin or the visual center of the bin. 
%Fortunately we can show that the total loss perceived is partitioned by these two components of loss, %and it holds (see appendix \ref{proof:partition} for a proof):
%\begin{eqnarray}\label{eqn:partition}
%L_s(\mathbf{x}) =  1/S^2_{\emptyset} \cdot \left(\sum_i^n L^N_i + \sum_j^{N_x} L^V_j\right)
%\end{eqnarray}
%-------

<<smoothing, echo=FALSE, include=FALSE, out.width="0.5\\textwidth", fig.width=5.4, fig.height=4.05>>=
set.seed(53022)
data <- data.frame(x=rnorm(200, 0, 2), y=rnorm(200, 0, 2), Freq=1)
data$xbin <- round(data$x)
data$ybin <- round(data$y)
data.means <- ddply(data, .(xbin,ybin), summarise, xmean=mean(x), ymean=mean(y))
data <- merge(data, data.means)

ggplot()+geom_histogram(aes(x=data$x, y=..count..), binwidth=1/4, fill=I("grey80"), colour=I("grey80"), alpha=I(.5)) + geom_histogram(aes(x=data$x, y=..count../4), binwidth=1, fill=NA, colour=I("grey50"), alpha=I(.5)) + theme_bw() + xlab("x") + ylab("Frequency") + ggtitle("Frequency Loss")
ggsave("./figure/FreqLoss.pdf", width=5.4*.8, height=4.05*.8)
@

\noindent

%The decomposition also holds for the random binning algorithm as well as the standard binning algorithm. 
%The expected total loss for either algorithm depends on the distribution the underlying data. 
%The total loss due to binning is $\sum_{i=1}^n (x_i - b(x_i))^2$, where in the standard algorithm, $b(x)$ maps $x$ to the closest bin center, and in the random algorithm, $b(x)$ is assigned probabilistically.

%---
% I believe we showed that this is only true when "rebinning" previously binned data and not deterministic when randomly binning at doubled bins from original data, thus not practically useful information so commented out
%It is worth noting that the loss function is a monotonically increasing function of binwidth. An interesting feature of the random binning algorithm is that the increase in loss that we experience by doubling the bin width is deterministic, even though the assignment of bins is not. This holds for repeated doubling of the bin width. A proof of this can be found in appendix \ref{proof:constantloss}. Doubling bin widths is, particularly in the framework of visualizations, a very natural step, making this an important property of the random binning.
%---

\begin{figure}[hbtp]
    \centering
		\subfloat[Numerical and Visual Loss]{\includegraphics[keepaspectratio=true, width=.46\textwidth]{./figure/Binning-DataVisualNumericalCenter.pdf}}
    \subfloat[Frequency Loss]{\includegraphics[keepaspectratio=true, width=.54\textwidth]{./figure/FreqLoss.pdf}}
\caption{\label{fig:stdBinTutorial2} Types of loss due to binning: (a) Numerical loss associated with the cumulative difference from the binned data to the numerical center and visual loss associated with the difference between the numerical and visual bin centers.  (b) Frequency loss associated with a change of bin width to 1 from 0.25 in the example of Figure~\ref{binning}.}   
	\end{figure}

\subsection{Loss of Frequency Information} 
Loss of frequency information in a scatterplot happens because of our inability to render elements of a third dimension efficiently. By employing a color scheme for filling tiles according to the observed frequencies corresponds to a binning of the frequencies into a few different levels, with losses stemming from the same source as the previously discussed numerical loss.
We will assume that we have a set of size $n_C$ of  frequency levels $\{c^\ast_k\}$ with $1 \le k \le n_C$. An observed frequency $c_j$ with $1 \le j \le N_x \times N_y$, where $N_x$ and $N_y$ are the number of bin divisions in the X and Y directions respectively,  is assigned to the closest frequency level $c^\ast_k = b(c_j)$.  
The \textit{unscaled} frequency loss is then defined as
\begin{eqnarray}\label{eqn:frequency}
L_\text{Freq} = \sum_{j=1}^{N_x N_y} \left(c_j - b(c_j)\right)^2
\end{eqnarray}

% When we consider loss of frequency information, we must consider both dimensions simultaneously, as frequency information is based on the combination of binning in $x$ and  $y$ direction. We define loss of frequency information as: \hh{what's that in words?}
% $$ L_\text{Freq} = \sum_j \left(c_j - \frac{1}{n_X n_Y}\sum_j c_j\right)^2,$$ 
% where $n_X, n_Y$ are the number of bins in $x$ and $y$, respectively. 

The loss of frequency information due to binning results from tiling over larger areas, thus results from {\it smoothing} the frequency variation (see Figure~\ref{fig:stdBinTutorial2}). 

Frequency data consists of counts, which most commonly exhibit skew densities, i.e. there are usually a lot of cells with small cell counts and a few cells with extremely large counts. 

A log transformation therefore produces a more symmetric distribution of frequency information, increasing perceptual resolution. This is consistent with the Weber-Fechner law which suggests that increased stimulus intensity is perceptually mapped on the log scale \citep{sp}. Using a logarithmic mapping of frequency to the color or size aesthetic provides a more natural perceptual experience and simultaneously increases the perceptual resolution of the graph. This suggests that frequency loss be calculated as 
\begin{eqnarray}\label{eqn:logfrequency}
L_{\log \text{Freq}} = \sum_{j=1}^{N_x N_y} \left(\log(c_j+1) - \log(b(c_j)+1)\right)^2
\end{eqnarray}

Note that we have added one count to every cell before taking the logarithm to avoid problems with empty cells. 

This numerical assessment of frequency loss does not account for limitations in human perceptual ability. Research suggests that under optimal conditions, we can effectively compare about seven colors \citep{colorperception}, which provides a physical upper limit on the amount of frequency variation we can perceive.
As a result, a binning width which produces fewer than seven frequency categories is preferable, while minimizing numerical loss within that constraint. 

To examine this further, we simulated points from three different distributions: Uniform, Normal, and Exponential to determine under what circumstances log frequency scaling is performing better than linear frequency scaling. Figure~\ref{fig:logcolor} gives an overview of this simulation. In the top row binned scatterplots with a color scheme based on linear frequency scaling is used, the bottom row shows the same data based on log frequency scaled colors.

For uniformly distributed data (right column) the choice of color scheme does not have a huge impact, but under both the normal and exponential distribution, log frequency scaling results in categories which account for more uniform proportions of the data (as can be seen by the color strips in the second and third row of Figure~\ref{fig:logcolor}). This is desirable because it provides more discernible frequency information. 
\begin{figure}[hbtp]
\centering
<<FrequencyCategories,echo=FALSE, results='hide', cache=TRUE>>=
options(stringsAsFactors=FALSE)
library(plyr)
library(ggplot2)
library(RColorBrewer)
set.seed(5032)
data <- rbind(data.frame(x=runif(1000, 0, 20), y=runif(1000, 0, 20), 
                         Freq=1, Distribution="Uniform"), 
              data.frame(x=rnorm(1000, 10, 3), y=rnorm(1000, 10, 3), 
                         Freq=1, Distribution="Normal"),
              data.frame(x=rexp(1000, .3), y=rexp(1000, .3), 
                         Freq=1, Distribution="Exponential"))
data <- data[which(abs(data$x-10)<10 & abs(data$y-10)<10),]
# qplot(data=data, x=x, y=y, geom="point")+facet_wrap(~Distribution)

data.bin <- data
data.bin[,1:3] <- round(data[,1:3])
data.bin <- ddply(data.bin, .(Distribution, x, y), summarise, Freq=sum(Freq))
data.bin <- data.bin[order(data.bin$Distribution, data.bin$Freq),]
# 
# 
# qplot(data=data.bin, x=Freq,geom="blank") + geom_density(aes(y=..scaled..),adjust=2) + facet_wrap(~Distribution, scales="free") + ylab("Density")
# 
# qplot(data=data.bin, x=Freq, geom="blank") + geom_density(aes(y=..scaled..), adjust=2) + facet_wrap(~Distribution,scales="free") + scale_x_continuous(trans="log", breaks=c(1, 5, 10, 20, 40, 60)) + ylab("Density")
# 

get.upper <- function(x){
  num <- sapply(x, function(i) strsplit(i, ",", fixed=TRUE)[[1]][2])
  num <- sapply(num, function(i) substr(i, 1, nchar(i)-1))
  return(as.numeric(num))
}
get.lower <- function(x){
  num <- sapply(x, function(i) strsplit(i, ",", fixed=TRUE)[[1]][1])
  num <- sapply(num, function(i) substr(i, 2, nchar(i)))
  return(as.numeric(num))
}

labelsToValues <- function(x){
  return(data.frame(lower=get.lower(x), upper=get.upper(x)))
}


linearbins <- cbind(data.bin, ddply(data.bin, .(Distribution), function(x) return(data.frame(fill=as.numeric(cut(x$Freq, 7)), label=as.character(cut(x$Freq,7)), id=1:nrow(x))))[,2:4])
linearbins$fill <- factor(linearbins$fill)

x <- subset(linearbins, Distribution=="Uniform" & fill==1)
linearbins2 <- ddply(linearbins, .(Distribution, fill),
                     function(x){
                       temp <- labelsToValues(x$label[1])
                       return(cbind(temp, height=nrow(x)/(temp$upper-temp$lower), label=unique(x$label)))
                     })
linearbins2 <- ddply(linearbins2, .(Distribution), transform, height=height/sum(height))



logbins <- cbind(data.bin, ddply(data.bin, .(Distribution), function(x) return(data.frame(fill=as.numeric(cut(log(x$Freq), 7)), label=as.character(cut(log(x$Freq),7)), id=1:nrow(x))))[,2:4])
logbins$fill <- factor(logbins$fill)

x <- subset(logbins, Distribution=="Uniform" & fill==1)
logbins2 <- ddply(logbins, .(Distribution, fill),
                  function(x){
                    temp <- exp(labelsToValues(x$label[1]))
                    newlabel <- paste(c("(", round(temp[1], 3), ",", round(temp[2], 3), "]"), sep="", collapse="")
                    return(cbind(temp, height=nrow(x)/(temp$upper-temp$lower), label=unique(x$label)))
                  })

logbins2 <- ddply(logbins2, .(Distribution), transform, height=height/sum(height))

colors <- brewer.pal(9, "Blues")[-c(1:2)]

multiplot <- function(..., plotlist=NULL, file, cols=1, layout=NULL, heights=NULL) {
  require(grid)
  
  # Make a list from the ... arguments and plotlist
  plots <- c(list(...), plotlist)
  
  numPlots = length(plots)
  
  # If layout is NULL, then use 'cols' to determine layout
  if (is.null(layout)) {
    # Make the panel
    # ncol: Number of columns of plots
    # nrow: Number of rows needed, calculated from # of cols
    nrow = ceiling(numPlots/cols)
    
    if (is.null(heights)) heights = unit(rep(1, numPlots), "null")
    layout <- matrix(seq(1, cols * ceiling(numPlots/cols)),
                     ncol = cols, nrow = nrow)
  }
  
  if (numPlots==1) {
    print(plots[[1]])
    
  } else {
    # Set up the page
    grid.newpage()
    pushViewport(viewport(layout = grid.layout(nrow(layout), ncol(layout), heights=heights)))
    
    # Make each plot, in the correct location
    for (i in 1:numPlots) {
      # Get the i,j matrix positions of the regions that contain this subplot
      matchidx <- as.data.frame(which(layout == i, arr.ind = TRUE))
      
      print(plots[[i]], vp = viewport(layout.pos.row = matchidx$row,
                                      layout.pos.col = matchidx$col))
    }
  }
}

p1 <- qplot(data=linearbins, x=x, y=y, geom="tile", fill=fill)+facet_wrap(~Distribution) +
  scale_fill_manual(values=colors, name="Frequency", guide="none") + theme_bw() +
  ggtitle("(a) Joint Density: Linear Binning") +  theme(plot.title = element_text(hjust=0, lineheight=.8, size=12), plot.margin = unit(c(0,0,0,0), "cm"))

X <- data.frame(fill=with(linearbins, rep(fill, Freq)), Distribution = with(linearbins, rep(Distribution, Freq)))
X <- ddply(X, .(Distribution), transform, id=1:length(Distribution))
p2 <- qplot(id, binwidth=1, fill=fill, data=X, alpha=I(1)) + 
  scale_fill_manual(values=colors, name="Frequency", guide="none") + 
  facet_grid(.~Distribution, scales="free") +
  xlab("Count") + theme_bw() + ylab("") +
  ggtitle("(b) Color Frequency: Linear Binning") + 
  theme(axis.text.y=element_text(colour="white"), axis.ticks=element_blank()) + 
  theme(plot.title = element_text(hjust=0, lineheight=.8, size=12), plot.margin = unit(c(0,0,0,-0.35), "cm"))
# linear binning
pdf(file="p2.pdf", width=8, height=3)
p2
dev.off()

# p2 <- qplot(data=linearbins2, xmin=lower, xmax=upper, ymin=0, ymax=height*1000, fill=fill, geom="blank", asp=1.5) + 
#   geom_rect(colour=I("grey50"), show_guide=FALSE, size=0.5)+
#   scale_fill_manual("Tile Color", values=colors)+
#   facet_wrap(~Distribution, scales="free") + 
#   theme_bw() + scale_x_continuous("Bin Frequency") + 
#   scale_y_continuous("") + ggtitle("(b) Distribution of Frequency: Linear Binning") + theme(plot.title = element_text(hjust=0, lineheight=.8, size=12), plot.margin = unit(c(0,0,0,0), "cm"))

# p4 <- qplot(data=logbins2, xmin=lower, xmax=upper, ymin=0, ymax=1000*height, fill=fill,  geom="blank", asp=1.5) + 
#   geom_rect(colour=I("grey50"), show_guide=FALSE, size=0.5)+
#   scale_fill_manual("Tile Color", values=colors)+
#   facet_wrap(~Distribution, scales="free") + 
#   theme_bw() + scale_x_continuous("Bin Frequency") + 
#   scale_y_continuous("") + ggtitle("(c) Distribution of Frequency: Log Binning") + theme(plot.title = element_text(hjust=0, lineheight=.8, size=12), plot.margin = unit(c(0,0,0,0), "cm"))

X <- data.frame(fill=with(logbins, rep(fill, Freq)), Distribution = with(logbins, rep(Distribution, Freq)))
X <- ddply(X, .(Distribution), transform, id=1:length(Distribution))
p4 <- qplot(id, binwidth=1, fill=fill, data=X, alpha=I(NULL)) + 
  scale_fill_manual(values=colors, name="Frequency", guide="none") + 
  facet_grid(.~Distribution, scales="free") +
  xlab("Count") + theme_bw() + ylab("") +
  ggtitle("(c) Color Frequency: Log Binning") + 
  theme(axis.text.y=element_text(colour="white"), axis.ticks=element_blank()) + 
  theme(plot.title = element_text(hjust=0, lineheight=.8, size=12), plot.margin = unit(c(0,0,0,-0.35), "cm"))


p3 <- qplot(data=logbins, x=x, y=y, geom="tile", fill=fill, asp=1.0)+facet_wrap(~Distribution) +
  scale_fill_manual(values=colors, name="Frequency", guide="none") + theme_bw() +
  ggtitle("(d) Joint Density: Log Binning") + theme(plot.title = element_text(hjust=0, lineheight=.8, size=12), plot.margin = unit(c(0,0,0,0), "cm"))

multiplot(p1, p2, p4, p3, cols=1, heights=c(2.2,1,1,2.2))
#ggsave("./figure/MultipleDensityFigure.png", width=8.5, height=11, dpi=1200)
@
%\includegraphics[width=\textwidth]{figure/MultipleDensityFigure.pdf}
\caption{\label{fig:logcolor} Linear and Log Frequency Binning  to Simulated Data from Uniform, Normal, and Exponential distributions}
\end{figure}

Frequency loss is much more difficult to normalize, as we must balance the partial loss of frequency information due to over-plotting with the total loss of frequency information that results if we use only a single bin to categorize the data. 
For the purposes of numerical assessment of frequency loss, we calculate the baseline frequency loss as the loss corresponding to a minimal bin width given by the smallest non-zero absolute difference of successive values in the original data set, as shown in Figure~\ref{binning}.


\paragraph{Traditional Scatterplots and Frequency Information:}

Using a minimal bin width, a binned scatterplot is comparable to a standard scatter plot, with bins shaded in a binary manner, as each unique observed value is located in a different bin. Alpha blending as used in Figure~\ref{fig:scatter-alpha}c extends the binary shading of a standard scatterplot to an implicit shading according to frequency. The shading is implicit because the range of frequency information is not scaled to the range of shading values, so that maximum color saturation is reached well before the maximum frequency, truncating the perceivable frequency information. 
By explicitly shading bins according to frequency, more information is preserved than in a traditional scatter plot, as the frequency domain provides visual weight to tiles which represent more points. This generalization allows us to describe the plots in Figure~\ref{fig:scatter-alpha}d and e under the same framework as plots a-c. 

By additionally increasing the bin width, we  provide increasingly higher-level summaries of the data by  smoothing over local structures. For small bin widths these structures are likely to be  {\it noise} inherent in any real data set, while removing structures associated with  larger bin widths likely masks real signal in the data.




\subsection{Recovery of Numerical Information}
% The total loss from reducing a value $x$ to bin center $x^\ast$ is at least as large as the loss from reducing points $x_i$ to $\overline x_i$, the numerical center of the binned data.
% % that's only true for the expected value - not for all individuals, but you don't need - we just use the partition. 

As shown before, the overall loss partitions naturally into visual and numerical loss (see equation \ref{eqn:partition}). We can make use of this and
 recover some of the numerical information lost due to binning by storing the numerical center $(\bar{x},\bar{y})$ instead of the visual center $(x^\ast,y^\ast)$ for use in subsequent calculations. Storing the binned data as the arithmetic mean rather than the visual center reduces the total loss, and the visual center can be recovered from the bin width and  bin center $(O_X, O_Y)$, see table \ref{RecoveryComparison} for a numeric example for this. 

<<recover,echo=FALSE,eval=FALSE,results='asis'>>=
set.seed(90)
x <- rnorm(100, 5, 2)
y <- rnorm(100, 5, 2)
d <- data.frame(x=x, y=y, c=1)
d <- d[order(d$x, d$y),]
print(xtable(head(d,10)[c(2,1,3,6,10,5,4,8,9,7),], digits=c(0, 4, 4, 0), caption=paste('Original Data\\newline', nrow(d), 'rows'), include.rownames=FALSE, table.placement="hbtp", floating=FALSE))
dbin <- ddply(binStd(d, c(1,1)), .(x,y), summarize, c=sum(c))
dbin <- dbin[order(dbin$x, dbin$y),]
print(xtable(head(dbin), digits=c(0, 0, 0, 0), caption=paste('Binned Data\\newline', nrow(dbin), 'rows'), align=c('X','X','X','c')), include.rownames=FALSE, table.placement="hbtp", floating=FALSE, tabular.environment="tabularx", width='.7\\textwidth')
names(d) <- c("x", "y", "Freq")
drecover <- unique(lossCalc(d, c(1,1), "standard", newData=TRUE)$NewData)
names(drecover) <- c("x", "y", "c")
drecover <- drecover[order(drecover$x, drecover$y),]
print(xtable(head(drecover), digits=c(0, 4, 4, 0), caption=paste('Recovered Data\\newline', nrow(drecover), 'rows'), include.rownames=FALSE, table.placement="hbtp", floating=FALSE))
@


\begin{table}[hbtp]
\begin{minipage}[t]{.33\textwidth}\centering\subfloat[Original Data, 100 rows]{
\begin{tabular}{rrr}\hline
       $x$ &      $y$ & c \\ \hline
 -0.2582 & 1.5497 & 1 \\ 
 -0.3843 & 5.6508 & 1 \\ 
 1.0597 & 2.4308 & 1 \\ 
 2.2680 & 1.5877 & 1 \\ 
 2.4992 & 4.6260 & 1 \\ 
 1.6788 & 5.0260 & 1 \\ 
 1.6054 & 6.4125 & 1 \\ 
 2.4180 & 6.4438 & 1 \\ 
 2.4345 & 6.4462 & 1 \\ 
 2.3620 & 10.3119 & 1 \\  \hline
\end{tabular}}
\end{minipage}\hfil
\begin{minipage}[t]{.33\textwidth}\centering\subfloat[Binned Data Visual Centers, 49 rows]{
\begin{tabularx}{.7\textwidth}{XXc} \hline
  $x^\ast$ & $y^\ast$ & c \\ \hline
  0 & 2 & 1 \\ 
  0 & 6 & 1 \\ 
  1 & 2 & 1 \\
  2 & 2 & 1 \\
  2 & 5 & 2 \\ 
  2 & 6 & 3 \\ 
  2 & 10 & 1 \\ 
   \hline
\end{tabularx}}
\end{minipage}\hfil
\begin{minipage}[t]{.33\textwidth}\centering\subfloat[Binned Data Numerical Centers, 49 rows]{
\begin{tabular}{rrc}\hline
  $\bar{x}$   &    $\bar{y}$ & c \\ \hline
 -0.3843 & 5.6508 & 1 \\ 
 -0.2582 & 1.5497 & 1 \\ 
  1.0597 & 2.4308 & 1 \\ 
  2.2680 & 1.5877 & 1 \\  
  2.0890 & 4.8260 & 2 \\ 
  2.1526 & 6.4341 & 3 \\ 
  2.2680 & 1.5877 & 1 \\ \hline
\end{tabular}}
\end{minipage}
\caption{\label{RecoveryComparison}ten rows from Original and Binned Data Tables, with data storage sizes}
\end{table}

Information recovery comes with a two-fold cost: the additional storage of numeric bin centers triples the amount of information stored, and the computational cost to compute numeric bin centers is O($nm^2$), where $n$ is the number of data points and $m^2$, the total number of bins.
O($nm^2$) $\approx$ O($n$), as $m$ is generally negligible compared to $n$.

In spite of the costs, information recovery is still useful for very large data, where the unbinned data set is  computationally intractable. Minimizing numerical loss in this manner provides for more accurate calculations from the data, while maintaining the storage space advantages of binning. %but creates an additional complexity in that the geometric center of the tile (the visual bin center) does not correspond to the stored, binned data. 
% The random binning algorithm serves as a less computationally intensive, unbiased alternative for 
% data sets which are too large to partition the loss in this manner.
% \hh{is that true, or does it require the expected value to be the visual center?}

\section{Discussion and Examples}
\subsection{Comparison of Loss in Binning Algorithms}
<<LossTableGraphs,echo=F,results="hide",cache=TRUE, eval=TRUE>>=
# bins <- rbind(c(1,1), c(2,2), c(2,5), c(5, 2), c(4,4), c(3, 5), c(5, 3), c(5, 5), c(5, 25), c(25, 5), c(2, 50), c(50, 2), c(5, 50))
computeLoss <- function(binning, data){
  lRandom <- lossCalc(data, binning=binning, type="random", newData = TRUE)
  lMean <- lossCalc(data, binning=binning, type="standard", newData=TRUE)
  datplot <- rbind(cbind(unique(lRandom$NewData), Algorithm="Random"), cbind(unique(lMean$NewData), Algorithm="Standard"))
  datplot$xbin <- round(datplot[,1]/binning[1],0)*binning[1]
  datplot$ybin <- round(datplot[,2]/binning[2],0)*binning[2]
  #datplot2 <- ddply(datplot, .(G, SO,Algorithm), summarize, count=length(G))
    qplot(xbin, ybin, geom="tile", data = datplot, fill = fsum , 
          ,main=paste("Baseball: Binning by (", binning[1], ", ", binning[2],")", sep="")) + facet_grid(.~Algorithm) +
        scale_fill_gradient(low="#56B1F7", high="#132B43", guide="legend", trans="log", breaks=c(1, 5, 50, 500, 5000)) + theme_bw() + theme(legend.position="bottom")
    ggsave(paste("./RandomStd", binning[1], binning[2], ".pdf", sep=""), width=8, height=5, units="in", dpi=1200)
  return(rbind(
    cbind(binX = binning[1], binY = binning[2], Function = "Random", lRandom$Loss), 
    cbind(binX = binning[1], binY = binning[2], Function = "Standard", lMean$Loss)))
}

bins <- rbind(c(2,2), c(5,25))
LossTable <- NULL      
LossTable <- do.call("rbind", lapply(1:nrow(bins), function(i) computeLoss(bins[i,], d1)))
LossTable <- as.data.frame(LossTable)
names(LossTable) <- c("binX", "binY", "Function", "Numerical Loss G", "Numerical Loss SO", "Visual Loss G", "Visual Loss SO", "Total Loss G", "Total Loss SO", "Frequency Loss")
tab <- xtable(LossTable, digits=c(0, 0, 0, 0, 8, 8, 8, 8, 8, 8, 8))
@

In order to quantify the advantages of each algorithm, we first compare the three sources of loss at different bin widths. The numerical and visual losses inherent in creating a binned scatterplot are a result of our desire to reduce and summarize the data until it is more manageable in size, and hence is a factor in our decision to bin the data in the first place.
%Figure~\ref{fig:binning-comparison} shows clearly that the standard algorithm can produce misleading graphical summaries. 
 The random algorithm has higher numeric and visual loss than the standard algorithm, but in many cases the binned scatterplots are visually indistinguishable. 
 
 The numerical loss is a function of the distribution and bin width, and once the bin width is chosen, it is entirely fixed. The random binning algorithm will inherently have a larger numerical loss than the standard algorithm because it allows for random assignment of points to numeric bin centers further from the original data than does the standard algorithm.  
 
 As the visual loss is the difference between the numerical center and visual center of the bin, it is a function of the distribution and bin width (and the specific random assignment, in the case of the random algorithm). We expect a larger visual loss under the random binning algorithms than in the standard binning because the numerical bin centers are allowed to be drawn further away from the visual center under the random binning algorithm.  

Frequency loss is argueably the most important source of loss, as it is perceivable in the variability of the shading of bins which has a large effect on the utility of the graphical data summary. The standard and random algorithms are very similar when we compare frequency loss. The random algorithm has more variability in allocating observed values to bin centers, but the impact of this variability on the loss frequency information is much less noticable than the increase seen in numerical loss. %~\ref{fig:LossComparison}  

Investigation of the loss sources we might be tempted to conclude that the standard binning algorithm is superior due to lower loss, however the random binning algorithm displays strong advantages when encountering problematic data structure issues. Random binning is especially advantagous when discrete data values are recorded at a consistant increments but we bin our data using a binwidth that is a non-integer multiple of the incremental widths. In this senario, we see that the non-synchronous data increments and bin widths will cause bins to contain unconsistant numbers of possible data value. Assuming that data values are spread uniformly over the discrete values we want a visualization that displays uniform frequency. Using the standard binning algorithm the misalignment of data increment and bin widths will lead to and artifical frequency discrepancy that manifests in a striped pattern in the visualization. The random binning algorithm is expected to display the intended uniform frequency pattern because the randomization of data values to bin centers mitigates the problematic bin width selection. 

%A similar situation can arise when discrete data values are recorded at consistant increments and binwidths are chosen that are a odd integer multiple of the data incremental widths. This causes the situation where every other bin will have a data value located on the upper bound.  Since the standard binning algorithm always bins data values on the boundary between bins to the lower bin center we will have an additional possible data value contributing to the frequency of every other bin, causing a striped pattern in our visualization. Again, the random binning algorithm mitigates this problem because it will allocate points on bin boundaries to adjacent bin centers with probability 0.5, leading to a more accurate display of frequency information.

\subsection{Binning Loss in Baseball Data: Strikeout and Game Counts}

For this data example we will revisit the baseball data used earlier in this paper.  The characteristics of loss described in the discussion above are exemplified by viewing the loss for the baseball data in Figure ~\ref{fig:LossComparison1}. We find that the visual and numerical loss are in fact higher while using the random binning algorithm, but that the two binning algorithms are almost indestiguishable in terms of frequency loss.  

Another thing to note about Figure ~\ref{fig:LossComparison1} is that under the standard binning algorithm the numerical loss is a monotone increasing function of the bin width, but visual loss is not a strictly increasing function of bin width.  This is appearant in the order of colored lines based on the Strike Out binning size.  This is because the numerical loss function sums loss over all observations, which means the distance from the points to the numerical center of bins can only increase as binwidth increases. Whereas the visual loss function sums loss over bin, thus as the binwidth increases the number of bins decreases which leads to the potential that the overall visual loss may decrease even though the distance from numerical centers to visual centers may increase for individual bins. 


\begin{figure}[hbtp]
\centering
<<LossComparison1, echo=FALSE, eval=T, fig.width=6.5, fig.height=8>>=

# connect <- dbConnect(dbDriver("MySQL"), user="2009Expo", 
#                      password="R R0cks", port=3306, dbname="baseball", 
#                      host="headnode.stat.iastate.edu")
# pitch <- new("dataDB", co=connect, table="Pitching")
# d1 <- dbData(pitch, vars=c( "G", "SO"))
# bins <- expand.grid(x=c(1:10, 15, 20), y=c(1:5, seq(10, 100, 10)))
# 
# library(multicore)
# lossesRdm <- do.call("rbind", mclapply(1:nrow(bins), function(i) lossCalc(data=d1, binning=c(bins[i,1], bins[i,2]), type="random")))
# lossesStd <- do.call("rbind", mclapply(1:nrow(bins), function(i) lossCalc(data=d1, binning=c(bins[i,1], bins[i,2]), type="standard")))
# 
# lossesRdm <- cbind(bins, lossesRdm)
# lossesStd <- cbind(bins, lossesStd)
# 
# losses <- rbind(cbind(Algorithm="Random", lossesRdm), cbind(Algorithm="Standard", lossesStd))
# write.csv(losses, "../dbData/data/losses.csv", row.names=FALSE)

losses <- read.csv("./data/losses.csv", stringsAsFactors=FALSE)
lossmelt <- melt(losses[,c(1:4,6,10)],id=c("Algorithm","x","y"))
lossmelt <- lossmelt[which(lossmelt$y == 1 | lossmelt$y == 10 | lossmelt$y == 50),]
lossmelt$y <- as.factor(lossmelt$y)
levels(lossmelt$variable) <- c("Numerical Loss","Visual Loss", "Log Frequency Loss")
qplot(x=x, y=value,
      group=interaction(y, Algorithm),
      data=lossmelt, geom="line",
      colour=y, size=I(1),
      xlab="Bin  Width  (Games)", ylab="Proportion Loss")+   
        theme_bw() + theme(legend.position="bottom") + 
        guides(colour = guide_legend(title="Bin Width (Strike Outs)", override.aes = list(size = 2))) +
        facet_grid(variable ~ Algorithm, scales = "free") 

# qplot(x=x, y=value,
#       group=interaction(y, Algorithm),
#       data=lossmelt, geom="line",
#       colour=y, size=I(1), linetype=Algorithm,
#       xlab="Bin  Width  (Games)", ylab="Proportion Loss")+   
#         theme_bw() + theme(legend.position="bottom") + 
#         guides(colour = guide_legend(title="Bin Width (Strike Outs)", override.aes = list(size = 2))) +
#         facet_grid(. ~ variable, scales = "free") 

@
\caption{\label{fig:LossComparison1} Loss for Baseball Data under Standard and Random Binning for different bin widths in the number of strike-outs and games.}
\end{figure}


%\textbf{Loss table for baseball example moved from beginning, this should not be its final resting place}
%\noindent
%Table \ref{tablelossexample} gives an overview of the size of all different sources of loss for the examples in figure~\ref{binning}.

<<tablelossexample, echo=FALSE, eval=FALSE, results='asis',cache=FALSE>>=
loss0.1 <- lossCalc(data=X[,c(1:2)], binning=c(.1, .1), type="standard", newData=TRUE)
loss0.25 <- lossCalc(data=X[,c(1:2)], binning=c(.25, .25), type="standard", newData=TRUE)
loss0.5 <- lossCalc(data=X[,c(1:2)], binning=c(.5, .5), type="standard", newData=TRUE)
loss1 <- lossCalc(data=X[,c(1:2)], binning=c(1, 1), type="standard",newData=TRUE)

loss0.1 <- data.frame(with(loss0.1, c(Loss, rows=nrow(ddply(NewData, .(x,y), summarise, Freq=sum(fsum))), cols=ncol(ddply(NewData, .(x,y), summarise, Freq=sum(fsum))))))
loss0.25 <- data.frame(with(loss0.25, c(Loss, rows=nrow(ddply(NewData, .(x,y), summarise, Freq=sum(fsum))), cols=ncol(ddply(NewData, .(x,y), summarise, Freq=sum(fsum))))))
loss0.5 <- data.frame(with(loss0.5, c(Loss, rows=nrow(ddply(NewData, .(x,y), summarise, Freq=sum(fsum))), cols=ncol(ddply(NewData, .(x,y), summarise, Freq=sum(fsum))))))
loss1 <- data.frame(with(loss1, c(Loss, rows=nrow(ddply(NewData, .(x,y), summarise, Freq=sum(fsum))), cols=ncol(ddply(NewData, .(x,y), summarise, Freq=sum(fsum))))))

table.loss <- data.frame(rbind(c(binwidth=0, TotalLoss.x=0, TotalLoss.y=0, TotalLoss.LogFreq=0, rows=200), 
                               c(binwidth=.1, loss0.1[5:7]*100, loss0.1[8]), 
                               c(binwidth=.25, loss0.25[5:7]*100, loss0.25[8]), 
                               c(binwidth=.5, loss0.5[5:7]*100, loss0.5[8]), 
                               c(binwidth=1, loss1[5:7]*100, loss1[8])))
names(table.loss) <- c("Bin Width", "% Loss in X", "% Loss in Y", "% Loss in log(Freq)", "Size of Data")
print(xtable(table.loss, digits=c(0, 2, 4, 4, 4, 0), caption="Loss due to binning for all examples in figure 4.", label="tablelossexample"), include.rownames=FALSE, table.placement="hbtp")
@


We may also use this example to highlight the advantages of random binning. The two binned scatterplots at the top of Figure~\ref{fig:binning-comparison} show bins of the same width, but clear differences between the binning results are evident. The standard binning algorithm has rounding artifacts that are clearly visible in the vertical strips. These vertical stripes are from the computational rounding issues encountered if we use the standard binning when we have the data values were recorded on an evenly spaces discrete scale (in our case, integers) and the binwidths are integer multiples of the data increments.  This reinforces the arguement that in practice the random algorithm can produce a graph that better represents the true shape of the data than the standard algorithm even if the numerical and visual loss is higher.


\textbf{Are we sure that this is because of an odd binwidths and not just that the bins were not centered on an integer?} In contrast, when the bin widths are odd, the two binning algorithms yield nearly indistinguishable binned scatterplots, as shown in the bottom row of Figure~\ref{fig:binning-comparison}.\textbf{I think that strips disappearing in (5,25) bins is because of where we START the binning not that the bin widths are even}

\begin{figure}[hbtp]
\centering
\subfloat[(2, 2) tiling of G, SO data]{
\includegraphics[keepaspectratio=true, width=.9\textwidth]{images/RandomStd22.pdf}
}\\
\subfloat[(5, 25) tiling of G, SO data]{
\includegraphics[keepaspectratio=true, width=.9\textwidth]{images/RandomStd525.pdf}
}
\caption{\label{fig:binning-comparison} Comparison of random and standard binning: standard binning introduces artificial striping when the chosen bin width is not a multiple of the data resolution.}
\end{figure}




\subsection{Big Data: Airline Departure Times}
The Federal Aviation Association (FAA) requires all airlines based in the United States to report   details for every single flight. These are published online by the Bureau of Transportation Services at \url{http://www.transtats.bts.gov/DataIndex.asp}. 
Every day there are about 16,000 flights across the United States adding up to almost 6 Million flights a year. Scheduled and actual departure times for all flights in 2011 make up --in uncompressed form-- a file of about 450 MB. A comparison of scheduled and actual departure times allows us an investigation of on-time performance of air carriers.

%We obtained airline scheduled departure times and actual departure times from the Department of Transportation for the year 2011. These times do not include actual departure date, however, so a flight scheduled to leave at 23:00 which is delayed by two hours is recorded as leaving at 01:00. The uncompressed data file is about 450 MB, and consists of two columns of almost six million records. 

Figure~\ref{airline-scatter} shows examples of two scatterplots of scheduled versus actual departure times. The plot on the left shows a sample of one million of those records. Even while using alpha blending this results in a severely over-plotted graph. On the right is a binned scatterplot of the complete data binned at 1-minute intervals. This does not have any spatial loss, as the times are recorded by minute. The 1-minute binning also reduces the data file to 176,384 individual records, less than 3\% of the original data. Both plots show the same big picture patterns: scheduled and actual arrival times are highly correlated, recognizable from the conglomeration of points along the line of identity.  
Scheduled departure times past 6 am in the morning are much more common than earlier flights. It is much more likely for a  flight to be delayed than to leave early,
leading to the wash-out effect above the line, that is getting thinner with increasing delays. The range of delays on usual days starts at about one hour at 6 am and increases during the day to about 2 hours. The few number of flights before 6 am are also visible in both plots.
The triangle of observations on the bottom right visible in both plots is nothing but an artifact of the data collection consisting of flights that are scheduled before midnight, but are delayed to departures past midnight. The cloud of outliers halfway between the two main structures is potentially interesting, since no immediate explanation comes to mind, and would be worthy of a follow-up investigation.
 What is not apparent in the plot on the left, is some fine-level structure that the plot based on all of the data shows. A close inspection of the plot on the right hand side reveals  darker colored vertical lines at 30 minute intervals. It is obvious that more flights are scheduled with departures on the hour and at 30 minutes past the hour. 


\begin{figure}[hbtp]\centering
%\subfloat[Million-point sample of Airline Data]{\includegraphics[width=.44\linewidth, keepaspectratio=TRUE]{images/MillionSample.pdf}}\hfil
%   Swap Back at later date (shrinks size of pdf to managable)
\subfloat[Million-point sample of Airline Data. \ktm{This plot is not compiling and needs to be reconstructed (my apologies)}]{\includegraphics[width=.44\linewidth, keepaspectratio=TRUE]{images/RplotFiller.pdf}}\hfil
\subfloat[Minimal binning of Airline Data]{\includegraphics[width=.56\linewidth, keepaspectratio=TRUE]{images/AirlineStdBinning1min.pdf}}
\caption{\label{airline-scatter}Scheduled and actual departure times of  flights across the United States in 2011. The plot on the left is based on a sample of the data, the plot on the right shows all flights. The big patterns are visible in both plots, but the plot on the left misses some of the finer level details in scheduling that is visible in the plot on the right.}
\end{figure}



\begin{figure}[hbtp]\centering
\includegraphics[keepaspectratio=true, height=.43\textheight]{images/AirlineStdBinning5mins.pdf}
\includegraphics[keepaspectratio=true, height=.43\textheight]{images/AirlineStdBinning15mins.pdf}
\caption{5-minute bins produce a higher-level summary of the data than shown in Figure~\ref{airline-scatter}b. 15-minute bins produce an even more coarse summary of the data.}\label{airline5mins-binning}
\end{figure}
Binning data by five-minute intervals produces a more high-level summary of the relationship between actual and scheduled departure time, though it necessarily obscures some of the finer details. In addition, binning data by five minute intervals reduces the size of the data set to a much more manageable 19,787 observations  which can be easily manipulated on probably any modern computer. Binning by 15-minute intervals reduces the data set to a nearly-trivial 3,575 observations, but the graphical summary becomes granular and less appealing at that resolution.

<<airlinesetup,echo=FALSE, eval=FALSE>>=
setwd("./data/")
# airline <- do.call("rbind", lapply(list.files(), function(i) read.csv(i)))
# write.csv(airline, "airline.csv", row.names=FALSE)
airline <- read.csv("airline.csv")
na.departure <- (is.na(airline[,8]) + is.na(airline[,9]))> 0
airline <- airline[!na.departure,8:9]

airline$CRS_DEP_TIME <- floor(airline$CRS_DEP_TIME/100)*60 + airline$CRS_DEP_TIME%%100
airline$DEP_TIME <- floor(airline$DEP_TIME/100)*60 + airline$DEP_TIME%%100

library(lubridate)
convert.time <- function(x){
  floor(x/60) + (x%%60)/60
}

library(multicore)

airline.freq <- ddply(airline, .(CRS_DEP_TIME, DEP_TIME), function(i) cbind(unique(i), Freq=nrow(i)))
write.csv(airline.freq, "ReducedAirlineData.csv", row.names=FALSE)
airline.freq <- read.csv("ReducedAirlineData.csv")
airline.freq.plot <- airline.freq
airline.freq.plot[,1] <- convert.time(airline.freq.plot[,1])
airline.freq.plot[,2] <- convert.time(airline.freq.plot[,2])
airline.freq.plot <- airline.freq.plot[order(airline.freq.plot$Freq),]
airline.dummy <- airline.freq.plot[1,]
airline.dummy[1,1] <- NA
qplot(data=airline.freq.plot, x=CRS_DEP_TIME, y=DEP_TIME, fill=Freq, asp=1, geom="blank", xlab="Scheduled Departure Time", ylab="Actual Departure Time", main="Airline Departure Times (1 minute bins)") + geom_rect(aes(xmin=CRS_DEP_TIME-3/60, xmax=CRS_DEP_TIME+3/60, ymin=DEP_TIME-3/60, ymax=DEP_TIME+3/60, fill=Freq), alpha=I(.5)) + geom_rect(data=airline.dummy, aes(xmin=CRS_DEP_TIME-3/60, xmax=CRS_DEP_TIME+3/60, ymin=DEP_TIME-3/60, ymax=DEP_TIME+3/60, fill=Freq), aes.inherit=FALSE) + theme_bw()+ theme(legend.position="right") +scale_fill_gradientn(colours=c("#56B1F7", "#132B43"), guide="legend", trans="log", breaks=c(1, 10, 100, 1000, 10000))+ scale_x_continuous(breaks=c(0, 6, 12, 18, 24)) + scale_y_continuous(breaks=c(0, 6, 12, 18, 24))
ggsave("AirlineStdBinning1min.pdf", width=7.5, height=6, dpi=2304)
    

library(ggplot2)
library(dbData)
air.sample <- airline.freq.plot[sample(1:nrow(airline.freq.plot), 1000000, replace=TRUE, prob=airline.freq.plot$Freq),1:2]

qplot(data=air.sample, x=CRS_DEP_TIME, y=DEP_TIME, geom="point", alpha=I(.05), xlab="Scheduled Departure Time", ylab="Actual Departure Time", main="Sampled Airline Departure Times") + theme_bw()+ theme(legend.position="right") + scale_x_continuous(breaks=c(0, 6, 12, 18, 24)) + scale_y_continuous(breaks=c(0, 6, 12, 18, 24))
ggsave("MillionSample.pdf", width=6, height=6, dpi=576)

air.binned5 <- binStd(airline.freq, c(5, 5))
air.binned5 <- ddply(air.binned5, .(CRS_DEP_TIME, DEP_TIME), function(i) cbind(convert.time(unique(i[,1:2])), Freq=sum(i$Freq)))
write.csv(air.binned5, "airlineBin5Std.csv", row.names=FALSE)

# air.binned5.rdm <- binRdm(cbind(airline, Freq=1), c(5, 5))
# air.binned5.rdm2 <- ddply(air.binned5.rdm, .(CRS_DEP_TIME, DEP_TIME), function(i) cbind(convert.time(unique(i[,1:2]), Freq=sum(i$Freq)))
# write.csv(air.binned5.rdm2, "airlineBin5Rdm.csv", row.names=FALSE)

air.binned5 <- read.csv("airlineBin5Std.csv")
qplot(data=air.binned5, x=CRS_DEP_TIME, y=DEP_TIME, fill=Freq, asp=1, geom="tile", xlab="Scheduled Departure Time", ylab="Actual Departure Time", main="Airline Departure Times (5 minute bins)") + theme_bw()+ theme(legend.position="bottom") +scale_fill_gradientn(colours=c("#56B1F7", "#132B43"), guide="legend", trans="log", breaks=c(1, 10, 100, 1000, 10000))+ scale_x_continuous(breaks=c(0, 6, 12, 18, 24)) + scale_y_continuous(breaks=c(0, 6, 12, 18, 24))
ggsave("AirlineStdBinning5mins.pdf", width=6, height=6, dpi=576)
# 
# qplot(data=air.binned5.rdm2, x=CRS_DEP_TIME, y=DEP_TIME, fill=Freq, asp=1, geom="tile", xlab="Scheduled Departure Time", ylab="Actual Departure Time", main="Airline Departure Times (5 minute bins)") + theme_bw()+ theme(legend.position="bottom") +scale_fill_gradient(low="#56B4FB", high="#183347", guide="legend", trans="log")
# ggsave("AirlineRdmBinning5mins.png", binned, width=6, height=6)

# air.binned <- data.frame(apply(airline, 2, function(i) 15*round(i/15, 0)))
# air.binned.rdm <- binRdm(airline.freq, c(15, 15))
# air.binned2 <- ddply(air.binned, .(CRS_DEP_TIME, DEP_TIME), function(i) cbind(unique(i), Freq=sum(i$Freq)))
# write.csv(air.binned2, "airlineBin15.csv", row.names=FALSE)
air.binned <- read.csv("airlineBin15.csv")
air.binned2 <- air.binned
air.binned2[,1] <- convert.time(air.binned[,1])
air.binned2[,2] <- convert.time(air.binned[,2])
air.binned.rdm <- read.csv("airlineBin15Rdm.csv")
air.binned2.rdm <- air.binned.rdm
air.binned2.rdm[,1] <- convert.time(air.binned.rdm[,1])
air.binned2.rdm[,2] <- convert.time(air.binned.rdm[,2])

# air.binned2.rdm <- ddply(air.binned.rdm, .(CRS_DEP_TIME, DEP_TIME), function(i) cbind(unique(i), Freq=sum(i$Freq)))
# write.csv(air.binned2.rdm, "airlineBin15Rdm.csv", row.names=FALSE)

binned <- qplot(data=air.binned2, x=CRS_DEP_TIME, y=DEP_TIME, fill=Freq, asp=1, geom="tile", xlab="Scheduled Departure Time", ylab="Actual Departure Time", main="Airline Departure Times (15 minute bins)") + theme_bw()+ theme(legend.position="bottom") +scale_fill_gradientn(colours=c("#56B1F7", "#132B43"), guide="legend", trans="log", breaks=c(1, 10, 100, 1000, 10000))+ scale_x_continuous(breaks=c(0, 6, 12, 18, 24)) + scale_y_continuous(breaks=c(0, 6, 12, 18, 24))
ggsave("AirlineStdBinning15mins.pdf", binned, width=6, height=6, dpi=576)

# binnedRdm <- qplot(data=air.binned2.rdm, x=CRS_DEP_TIME, y=DEP_TIME, fill=Freq, asp=1, geom="tile", xlab="Scheduled Departure Time", ylab="Actual Departure Time", main="Airline Departure Times (15 minute bins)") + theme_bw()+ theme(legend.position="bottom") +scale_fill_gradientn(colours=c("#56B1F7", "#132B43"), guide="legend", trans="log", breaks=c(1, 10, 100, 1000, 10000))+ scale_x_continuous(breaks=c(0, 6, 12, 18, 24)) + scale_y_continuous(breaks=c(0, 6, 12, 18, 24))
# ggsave("AirlineRdmBinning15mins.png", binnedRdm, width=8, height=8, dpi=600)
@
\section{Conclusion}
Large Data sets of continuous variables are very difficult to visualize in raw form, due to over-plotting of points. Binning allows for the visualization and manipulation of large data sets, and easily translates into binned scatterplots which are more appropriate for the human visual system. The loss due to binning is the cost of reducing the data set to a more manageable size, but some of that loss can be recovered with additional computational investment.

We have presented two algorithms for binning; the standard algorithm, which has rounding artifacts but generally lower loss, and the random algorithm, which is a fast version of linear binning and is somewhat more computationally efficient. The choice of algorithm is a trade-off between speed and accuracy, and may be highly data set dependent. It is possible that there are computational modifications which could improve the accuracy of the random algorithm, but these modifications are likely to require additional computational operations.

% \begin{subsection}{Numerical Comparison of Binning Algorithms}
<<PrintTable,echo=F,include=FALSE, dependson='LossTableGraphs'>>=
  print(tab, include.rownames=FALSE)
@
% \end{subsection}

\begin{appendix}
\section{Mathematical Considerations}
\subsection{Partition of Spatial Loss}\label{proof:partition}
We want to show that spatial loss can be written as a partition of the form 
\[L_s(x) = 1/S^2_{\emptyset}\sum_i S^2_i = 1/S^2_{\emptyset}\left[\sum_i L^N_i + \sum_i L^V_i\right]\] 

For that let us consider the values $\{x_{i}: b(x_i) = x^\ast\}$ in a single bin with center $x^\ast$.
The numerical center of these points is given as  $1/n \sum x_i = \overline x$.
Then the total loss for each element $x_i$ in this bin is:

\begin{align*}
S_i^2 = \sum (x_i - x_i^\ast)^2 & = \sum (x_i - \overline x + \overline x - x_i^\ast)^2\\
& = \sum(x_i-\overline x)^2 + 2\sum (x_i-\overline x)(\overline x-x_i^\ast) + \sum(\overline x-x_i^\ast)^2\\
& = \sum(x_i - \overline x)^2 + 2\sum(0)(\overline x-x_i^\ast) + \sum(\overline x-x_i^\ast)^2\\
& = \sum(x_i - \overline x)^2 + \sum(\overline x-x_i^\ast)^2\\
& = L^N_i + L^V_i
\end{align*}
Adding over all elements in the bin and all bins gives the formula above.

\subsection{Constancy of Loss under Binary Splits}\label{proof:constantloss}
We want to show that under random binning the loss due to doubling of bin width is deterministic.

Let us consider the contribution $L_i$ of a single point $x_i$ to the loss when starting with the minimal bin width, i.e. each unique point is in a separate bin:
doubling the bin width leads to two  scenarios: 
a point is either at  the bin center or directly between two bins.
In the case that the point is at the bin center, its contribution to the total overall loss is zero,  $L_i = 0$, and the associated probability $p_i = 1$. 

Alternately, when the point is exactly half-way between two bin centers, the point is attributed with probability $p_i = \frac{1}{2}$ to either bin, leading to a loss of $L_i = \frac{1}{2} c_i$ regardless of which bin is assigned. Hence, in this case, loss is also entirely independent of which bin is assigned.
\end{appendix}
\bibliographystyle{asa}
\bibliography{references}
\end{document}
